# Automatic Outlier Detection

**Anomaly detection** is focused on identifying data objects that behave differently from expectations. It could be influenced by bad practices like noise, errors or some unexpected events. Unusual data also can be due to rare, but correct, behavior, which often result in interesting findings, motivating further investigation. Because of these reasons it is necessary to develop techniques that allow us to identify such unusual events. We assume that such events my induce some objects generated by a ”different mechanism”, which indicates that these objects might contain unexpected patterns that do not conform to a normal behavior. In statistics and machine learning areas we used to call such objects as anomalies \(or outliers\) which we want to identify accordingly. 

At the moment there exist many techniques to deal with problem of outliers. Initially each method was invented to deal with a specific case of abnormalities utilizing different properties of data. From the first sight, anomaly detection is perceived as a classification task to differentaite between normal and abnormal events. However, we usually have very small number of such abnormalities or do not have them at all, which transforms the problem of outlier detection into the unsupervised context, the type of machine learning, a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. For all that reasons it becomes very complicated to establish the most relevant algorithm for each case. 

Hydropshere creates automatically an outlier detection \(Auto OD\) metric for each model with uploaded training data, which assigns an outlier score for each request. A request is labeled as an outlier if the outlier score is greater than the 97th percentile of training data outlier scores distribution.

![](../../.gitbook/assets/auto_od_feature%20%281%29%20%281%29.gif)

You can observe those models deployed as metrics in your monitoring dashboard. These metrics provide you with information about how novel/anomalous your data is. If these values of the metric deviate significantly from the average, you can tell that you experience some potential abnomality event. In the case, if you observe a gradually increasing number of such events, then it might be associated with a data drift, which makes a need to re-evaluate your ML pipeline to check for errors.

## Supported Models

Right now Auto OD feature works only for Models with numerical scalar fields and uploaded training data.

